\chapter{Literature Review}
Over the last few decades, object detection has evolved into a central focal point of modern computer vision. The Researchers have constantly refined the underlying methods, moving from manual inspection and threshold-based techniques to state-of-the-art neural networks that learn complex representations directly from raw data. In particular, our constant hunt for both accuracy and efficiency have driven a wide range of innovations. These methods are now being applied to underwater environments, where conditions such as low light, turbidity, and the unpredictable motions of marine life create unusually challenging scenarios. The sections that follow provide a comprehensive overview of the historical foundations of object detection, the rise of Convolutional Neural Networks (CNNs) and Transformer-based models, and the context-specific issues that emerge when these techniques are deployed in marine domains.
\section{Early Approaches \& Sonar}
\subsection{Manual Methods \& Thresholding}
Traditional object detection, especially in industrial or surveillance applications, initially depended on manual checks or simplistic automated routines. In assembly lines, for instance, human inspectors or technicians would scan products for visible defects. While cost-effective in stable settings, these procedures could not scale when scenes became crowded or inspection periods lengthened. Early algorithmic solutions often consisted of threshold-based rules that identified significant deviations from a baseline image or background. Although these methods proved useful for consistently lit, low-clutter environments, they lost effectiveness in dynamic conditions or when objects partially overlapped.

\subsection{Sonar for Underwater Exploration}
Underwater object detection relied heavily on sonar for many years. Sonar systems transmit acoustic pulses and measure returning echoes to map seafloors, locate large aquatic animals, or track subsea structures. Although sonar provides coarse localisation in even the darkest or murkiest waters, it usually cannot distinguish subtle differences between organisms or detect small-scale anomalies. This limitation has motivated the exploration of camera-based detection in underwater research, especially for tasks such as identifying specific fish species in aquaculture facilities. While some attempts have been made to fuse sonar readings with optical imagery, reconciling these vastly different data types often proves complicated.
\subsection{Feature-Based Computer Vision}
Before the advent of deep learning, researchers turned to more advanced feature-based algorithms. Approaches like edge detection, optical flow, and background subtraction improved upon raw thresholding by isolating motion or well-defined object boundaries. Eventually, descriptors such as Scale-Invariant Feature Transform (SIFT) and Histogram of Oriented Gradients (HOG) allowed more robust detection by capturing gradients and corners across different scales or angles. These pipelines coupled with machine learning classifiers including Support Vector Machines (SVM) did better in different conditions but they were still a far cry from being independent and required extensive feature engineering. These algorithms were often rendered useless in underwater contexts by subtle shifts in colour or illumination, resulting in frequent misclassifications unless they were meticulously tuned.

\section{Deep Learning in Object Detection}
\subsection{LeNet-5: Foundations}
Convolutional Neural Networks (CNNs) gained recognition through the work of Yann LeCun and collaborators, particularly on LeNet-5 for digit classification. Though limited by computing capabilities at the time, LeNet-5 demonstrated that networks featuring convolutional and pooling layers could learn relevant features from pixels directly. This approach outperformed many handcrafted pipelines, setting the stage for deeper architectures once GPU hardware and larger datasets became accessible.
\subsection{AlexNet \& Advances}
The moment that redefined deep learning arrived with AlexNet, which won the 2012 ImageNet competition by a substantial margin. By harnessing GPU acceleration and ReLU activations, AlexNet showed how deep CNNs could surpass established methods when trained on high-volume image datasets. Subsequent architectures, including ZFNet, GoogLeNet, VGGNet, and ResNet, expanded on these ideas by adjusting kernel sizes, layer counts, or connection patterns. Researchers quickly realised that these convolutional backbones were not limited to classification; they could also be adapted for tasks like segmentation and object detection, fueled by bounding-box regression and region proposal strategies.
\section{Region-Based CNNs}
\subsection{From R-CNN to Faster R-CNN}
Region-Based Convolutional Neural Networks (R-CNN) represented a turning point in object detection. Initially, R-CNN generated bounding-box proposals through selective search, then employed a CNN to classify each proposal. Although significantly more accurate than earlier techniques, the process was computationally intensive since each proposal required a separate pass through the network. Fast R-CNN addressed this inefficiency by sharing convolutional computations and using a region of interest pooling strategy, thereby accelerating inference. Faster R-CNN further refined this approach through the introduction of a Region Proposal Network (RPN), which produced bounding-box suggestions with minimal overhead. Faster R-CNN went on to establish leading performance on well-known datasets like Pascal VOC and MS COCO, blending high accuracy with reasonable speed.

\subsection{Underwater Applications}
Region-based CNNs did not remain confined to terrestrial imagery. A study by Han et al. illustrated the effectiveness of Faster R-CNN in classifying marine wildlife captured in underwater video. Despite uneven lighting and challenging water conditions, they achieved a mean Average Precision (mAP) of 88.6 per cent at an IoU threshold of 0.5. This demonstrated how region-based methods, when trained with domain-specific data, could handle many of the complexities typically encountered in marine ecosystems. Sonar might remain valuable for broader-scale surveys, but region-based CNNs excelled at identifying individual fish or other organisms with fine-grained accuracy.

\section{Single-Shot \& Real-Time Frameworks}
\subsection{Speed in Detection}
Although region-based models reached high precision, their multi-stage workflows often limited real-time viability. In contexts such as autonomous driving or continuous underwater footage analysis, inference speed becomes critical. Single-shot detectors, which unify bounding-box predictions and classification within a single network pass, arose as a direct response to the need for higher throughput.
\subsection{YOLO, SSD, \& Improvements}
YOLO (You Only Look Once) sets the stage by treating detection as a regression problem that divides the input image into grids. Each cell in the grid predicts bounding boxes and class probabilities simultaneously. This design improved speed considerably, although initial versions struggled with smaller objects or heavy clutter. The Single Shot MultiBox Detector (SSD) approached detection by generating multiple feature maps, each specialised for objects at different scales. Over time, refinements like YOLOv3, YOLOv4, and YOLOv5 incorporated advanced data augmentation, robust backbones, and refined anchor box mechanisms to narrow the gap between single-stage and region-based detectors in terms of accuracy. The ability to achieve tens of frames per second or more made these models attractive to tasks that require immediate responses, including real-time aquaculture monitoring, where each second of delay could lead to missed behavioural cues or disease indications.
\section{Underwater Detection: Sonar \& Optical}
The ability of sonar has remained critical for broad-scale underwater mapping and for locating large objects in murky conditions. Yet, its limited resolution hampers fine-grained classification. Optical methods display superior performance for detecting smaller details, including morphological features and colour patterns, if water visibility is sufficient. Some measures are trying to combine sonar outputs with optical bounding boxes, which might increase the overall reliability. Challenges include reconciling disparate coordinate frames, uneven frame rates, and acoustic noise. Optical-based deep learning has become an invaluable tool in marine conservation and fish population studies, enabling precise species identification and behavioural analysis—capabilities that sonar data alone cannot provide.

\section{Video Tracking Techniques}
\subsection{Classical Tracking}
When object detection moves into multi-frame contexts, an additional layer of complexity arises. Traditional video trackers, including feature-based (SIFT, HOG key points) and model-based (shape fitting over time), attempt to maintain object identities from one frame to the next. Underwater environments, however, amplify the risk of occlusion by sediment or algae and can involve irregular lighting shifts caused by currents. Classic trackers frequently require elaborate parameter tuning to remain stable in such fluid conditions.
\subsection{Deep Learning Tracking}
Modern pipelines often marry deep learning detection with tracking modules to ensure stable identification across frames. A CNN or Transformer-based detector might identify each fish in every video frame, while a re-identification system (ReID) assigns consistent IDs. Although computationally expensive, this approach supports long-term observations of fish behaviour or coral reef dynamics.Aquaculture can show what is happening with feeding or early indicators of disease. Thus, the synergy between detection and tracking can provide more ecological information and a more intense marine intervention.
\section{Transformers in Vision}
\subsection{Self-Attention}
Transformers initially transformed natural language processing by allowing global attention to every position in a sentence, removing the constraints of recurrent networks. In vision, these ideas surfaced through Vision Transformers (ViT), splitting images into patches and applying multi-head attention. This pattern captures far-ranging relationships in a more direct way than CNNs, which rely on gradually expanding receptive fields.
\subsection{DETR, Swin, \& Underwater Relevance}
A Detection Transformer (DETR) was innovated by reimagining detection as a set prediction problem. It circumvented region proposals and non-maximum suppression but at the cost of long training cycles and a strong need for large data volumes. The Swin Transformer introduced a hierarchical window-based approach to manage computational load, allowing improved scalability to higher-resolution images. Transformers can be valuable in cluttered or large-scale underwater scenes, where global context might clarify which objects are relevant. However, their memory footprints, along with the limited availability of extensive labelled underwater datasets, make direct application less trivial. Domain adaptation methods are
\section{Hybrid Methods}
\subsection{CNN \& Transformer Integration}
Hybrid models attempt to merge CNN-based local feature extraction with the global self-attention of Transformers. For instance, a CNN might quickly encode lower-level shapes or textures, followed by a Transformer stage that refines bounding boxes using distant correlations in the image. Preliminary studies indicate that such hybrids can surpass pure CNN or Transformer setups in especially dense object scenarios. Nevertheless, optimising such systems is challenging since two totally different learning paradigms must be reconciled.
\subsection{Ensemble Methods}
Another research approach involves combining multiple detectors into an ensemble. A Transformer-based model can complement a CNN-based one, with their predictions integrated through techniques like bounding-box voting or weighted averaging. This strategy frequently raises accuracy but also multiplies computational requirements, typically making ensembles more suitable for offline analysis than for real-time operations. Although this can be invaluable when analysing archived footage of coral reefs to catalogue biodiversity, it may be impractical for a real-time aquaculture setup where hardware is constrained and continuous monitoring is necessary.
\section{Datasets \& Underwater Data}
\subsection{Mainstream Datasets}
There are few widely used datasets like MS COCO, Pascal VOC, and Open Images have dominated object detection research, providing diverse categories and ground-truth annotations. They act as universal benchmarks that allow model comparisons. Yet, these collections rarely account for the environmental shifts or specialised objects found underwater. Models trained on them often struggle with the extreme lighting variations and rich biological diversity characteristic of marine environments.
\subsection{Domain-Specific Collections}
Since underwater environments come with unique challenges, specialised datasets have been created to account for changing water clarity, shifting light conditions, and interactions between different species. These datasets usually contain labelled images of fish, algae, and even man-made structures like cages or mooring lines. While they are essential for training models that work well in aquatic settings, they often have limitations—such as being small in size or having class imbalances, where common fish species appear frequently while rarer ones are underrepresented.
\subsection{Training Implications}
When AI models are trained on images from land environments, they often struggle with underwater scenes, meaning they need extra adjustments to work properly. One way to help is through data augmentation, which involves adding effects like colour shifts or noise to make the images resemble real underwater conditions. However, fully adapting these models to marine environments is rarely straightforward. Since collecting and labelling underwater data takes a lot of time and money, collaboration between AI researchers and marine scientists is essential to combine their expertise and improve model accuracy.
\section{Bias, Fairness \& Ethics}
\subsection{Sources of Bias}
AI models can be biased if the training data doesn’t include enough variety in species or locations. For example, if a dataset mostly features a few common fish types, the model might struggle to recognize rarer species, which are important for biodiversity studies. Labeling mistakes can also be a problem—especially in images with lots of overlapping objects, where some fish might not be properly marked. In aquaculture, a model trained on data from one region might do well at spotting local issues but fail to detect diseases or conditions that are more common elsewhere.
\subsection{Real-World Impact}
Bias can distort estimates of fish populations, hinder invasive species detection, or affect feeding strategies. These problems might have ecological or commercial ramifications if decision-makers rely heavily on automated detections. Ethical considerations in AI research, which have primarily centred on human-focused applications, are slowly being extended to environmental concerns. Ensuring balanced coverage of species and habitats thus remains a priority.
\subsection{Mitigation Strategies}
Efforts to mitigate Bias include active sampling, targeted data collection, and synthetic oversampling of rare classes. Periodic revalidation of model outputs in changing marine ecosystems also helps pinpoint persistent classification errors. A transparent approach to documenting dataset composition and model performance is key for maintaining trust in the resulting technology, whether used for conservation programs or commercial aquaculture operations.
\section{Performance Metrics \& Constraints}
\subsection{Accuracy Metrics}
Mean Average Precision (mAP) is widely recognised as the benchmark for object detection accuracy at various Intersections over Union (IoU) thresholds. Evaluations sometimes distinguish performance on small, medium, or large objects, revealing whether certain scales pose recurring issues. In multi-frame scenarios, metrics like Multiple Object Tracking Accuracy (MOTA) may be added to assess the consistency of detection across consecutive frames
\subsection{Speed \& Resources}
Real-time demands prompt researchers to measure frames per second (FPS) or milliseconds per inference. If a system processes fewer than 10–15 FPS, it might not work well for fast-moving situations, though this isn’t a big issue for offline or pre-recorded video analysis. Hardware also plays a huge role—Transformers usually need more memory and computing power compared to CNNs, which can be simplified (pruned or quantised) to work better on smaller devices. In underwater conditions, things like battery life and data transfer limits can make it necessary to use lighter, more specialised models that tend to be more efficient.
\subsection{Underwater Adaptation}
Underwater cameras capture video in constantly changing conditions, like shifts in lighting and water clarity (turbidity). A model that works well in one situation might not perform the same in another unless it is retrained or adjusted. That is why testing these models in different environments is crucial before actually using them. Variability is lower in controlled settings like fish farms, where lighting is stable, and pens keep fish in one place. In contrast, open ocean studies show extremely more challenges. Environmental conditions can shift dramatically between daytime and nighttime or at varying depths, making accurate detection much more difficult.
\section{Conclusions}
Object detection has evolved from basic threshold-based methods to advanced deep-learning models that can automatically identify detailed features in images. Early systems depended heavily on hand-engineered workflows, but the arrival of Convolutional Neural Networks, as demonstrated by LeNet-5, AlexNet, and ResNet, reduced reliance on custom feature engineering. Region-based frameworks like R-CNN and Faster R-CNN further boosted accuracy by focusing on proposed bounding boxes, while one-stage detectors such as YOLO and SSD addressed real-time performance requirements.
In underwater contexts, factors like visibility fluctuations, background clutter, and limited annotated data complicate detection. Sonar remains helpful for broad scanning but provides less detail than optical strategies, which can pinpoint marine species at finer scales. More recent Transformer-based detectors, including DETR and Swin, offer global context modeling but demand large datasets and careful tuning. Ultimately, task constraints ranging from inference speed to ecological considerations influence which model type suits a particular underwater setting. The next chapter outlines how this project will compare CNN-based, Transformer-based, and hybrid approaches, with special attention to the practicalities of real-world deployments in aquatic environments.

