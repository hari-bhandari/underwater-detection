{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-31T09:58:01.591949Z",
     "start_time": "2025-01-31T09:58:01.475455Z"
    }
   },
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import cv2  # OpenCV for image processing\n",
    "\n",
    "# =============================================================================\n",
    "# 1. PATHS AND DATA SETUP\n",
    "# =============================================================================\n",
    "notebook_dir = Path.cwd()\n",
    "project_root = notebook_dir.parent\n",
    "data_path = project_root / \"data\"\n",
    "train_path = data_path / \"train/images\"\n",
    "labels_path = data_path / \"train/labels\"\n",
    "\n",
    "# Collect all image and annotation files\n",
    "image_files = list(train_path.glob(\"*.jpg\")) + list(train_path.glob(\"*.png\"))\n",
    "annotation_files = list(labels_path.glob(\"*.txt\"))\n",
    "\n",
    "# Class names\n",
    "class_names = ['crab', 'fish', 'jellyfish', 'shrimp', 'small_fish', 'starfish']\n",
    "\n",
    "# =============================================================================\n",
    "# 2. YOLO ANNOTATION PARSING\n",
    "# =============================================================================\n",
    "def parse_yolo_annotations(annotation_path, image_width, image_height):\n",
    "    \"\"\"\n",
    "    Parses a YOLO-style annotation file and converts normalized bbox coordinates\n",
    "    into pixel-based bounding boxes (xmin, ymin, xmax, ymax).\n",
    "    \"\"\"\n",
    "    bboxes = []\n",
    "    with open(annotation_path, \"r\") as file:\n",
    "        for line in file.readlines():\n",
    "            data = line.strip().split()\n",
    "            class_id = int(data[0])\n",
    "            x_center, y_center, width, height = map(float, data[1:])\n",
    "\n",
    "            xmin = int((x_center - width / 2) * image_width)\n",
    "            ymin = int((y_center - height / 2) * image_height)\n",
    "            xmax = int((x_center + width / 2) * image_width)\n",
    "            ymax = int((y_center + height / 2) * image_height)\n",
    "\n",
    "            bboxes.append({\n",
    "                \"name\": class_names[class_id],\n",
    "                \"xmin\": xmin,\n",
    "                \"ymin\": ymin,\n",
    "                \"xmax\": xmax,\n",
    "                \"ymax\": ymax\n",
    "            })\n",
    "    return bboxes\n",
    "\n",
    "# =============================================================================\n",
    "# 3. DRAW BOUNDING BOXES (ON PIL IMAGES)\n",
    "# =============================================================================\n",
    "def draw_bounding_boxes(pil_image, bboxes, color=\"red\"):\n",
    "    \"\"\"\n",
    "    Draws bounding boxes and labels on a copy of the input PIL image.\n",
    "    Returns a new PIL Image with bounding boxes.\n",
    "    \"\"\"\n",
    "    draw_image = pil_image.copy()\n",
    "    draw = ImageDraw.Draw(draw_image)\n",
    "\n",
    "    # Attempt to load a TTF font; if unavailable, fall back to default\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"DejaVuSans-Bold.ttf\", 20)\n",
    "    except IOError:\n",
    "        font = ImageFont.load_default()\n",
    "\n",
    "    for bbox in bboxes:\n",
    "        # Draw rectangle\n",
    "        draw.rectangle(\n",
    "            [(bbox[\"xmin\"], bbox[\"ymin\"]), (bbox[\"xmax\"], bbox[\"ymax\"])],\n",
    "            outline=color,\n",
    "            width=3\n",
    "        )\n",
    "        # Draw label\n",
    "        text_position = (bbox[\"xmin\"], max(0, bbox[\"ymin\"] - 20))\n",
    "        draw.text(text_position, bbox[\"name\"], fill=color, font=font)\n",
    "\n",
    "    return draw_image\n",
    "\n",
    "# =============================================================================\n",
    "# 4. PREPROCESSING METHODS FOR UNDERWATER IMAGES\n",
    "# =============================================================================\n",
    "\n",
    "def white_balance_bgr(cv_bgr):\n",
    "    \"\"\"\n",
    "    Applies a simple \"gray-world\" white balance to a BGR image (OpenCV format).\n",
    "    Returns the corrected BGR image.\n",
    "    \"\"\"\n",
    "    # Convert to float for precision\n",
    "    result = cv_bgr.astype(np.float32) / 255.0\n",
    "\n",
    "    # Compute mean of each channel\n",
    "    avgB = np.mean(result[:,:,0])\n",
    "    avgG = np.mean(result[:,:,1])\n",
    "    avgR = np.mean(result[:,:,2])\n",
    "\n",
    "    # Gray-world assumption\n",
    "    grayValue = (avgB + avgG + avgR) / 3\n",
    "    result[:,:,0] *= (grayValue / (avgB + 1e-8))\n",
    "    result[:,:,1] *= (grayValue / (avgG + 1e-8))\n",
    "    result[:,:,2] *= (grayValue / (avgR + 1e-8))\n",
    "\n",
    "    # Clip to [0, 1], then scale back to [0, 255]\n",
    "    result = np.clip(result, 0, 1)\n",
    "    result = (result * 255).astype(np.uint8)\n",
    "    return result\n",
    "\n",
    "def enhance_clahe_bgr(cv_bgr, clip_limit=2.0, grid_size=(8, 8)):\n",
    "    \"\"\"\n",
    "    Enhances a BGR image (OpenCV) using CLAHE in LAB color space\n",
    "    to improve contrast in underwater images.\n",
    "    \"\"\"\n",
    "    # Convert to LAB color space\n",
    "    lab = cv2.cvtColor(cv_bgr, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "\n",
    "    # Apply CLAHE on the L-channel\n",
    "    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=grid_size)\n",
    "    cl = clahe.apply(l)\n",
    "\n",
    "    # Merge and convert back to BGR\n",
    "    limg = cv2.merge((cl, a, b))\n",
    "    enhanced_bgr = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n",
    "    return enhanced_bgr\n",
    "\n",
    "def gamma_correction_bgr(cv_bgr, gamma=1.2):\n",
    "    \"\"\"\n",
    "    Applies gamma correction to a BGR image (OpenCV).\n",
    "    Gamma > 1.0 brightens the image, while Gamma < 1.0 darkens it.\n",
    "    \"\"\"\n",
    "    # Build a lookup table for gamma values [0..255]\n",
    "    invGamma = 1.0 / gamma\n",
    "    table = np.array([\n",
    "        ((i / 255.0) ** invGamma) * 255\n",
    "        for i in np.arange(256)\n",
    "    ]).astype(\"uint8\")\n",
    "\n",
    "    # Apply gamma correction using the lookup table\n",
    "    return cv2.LUT(cv_bgr, table)\n",
    "\n",
    "def pil_to_bgr(pil_img):\n",
    "    \"\"\"\n",
    "    Converts a PIL (RGB) image into OpenCV BGR format.\n",
    "    \"\"\"\n",
    "    cv_image = np.array(pil_img)\n",
    "    cv_image = cv_image[:, :, ::-1].copy()  # RGB -> BGR\n",
    "    return cv_image\n",
    "\n",
    "def bgr_to_pil(cv_bgr):\n",
    "    \"\"\"\n",
    "    Converts an OpenCV BGR image into a PIL (RGB) image.\n",
    "    \"\"\"\n",
    "    cv_rgb = cv2.cvtColor(cv_bgr, cv2.COLOR_BGR2RGB)\n",
    "    pil_image = Image.fromarray(cv_rgb)\n",
    "    return pil_image\n",
    "\n",
    "# =============================================================================\n",
    "# 5. MAIN FUNCTION: SHOW ORIGINAL + MULTIPLE PREPROCESSED VERSIONS\n",
    "# =============================================================================\n",
    "def display_random_image_with_different_preprocessing():\n",
    "    \"\"\"\n",
    "    - Loads a random image from the dataset.\n",
    "    - Parses YOLO bounding boxes.\n",
    "    - Shows four images side by side:\n",
    "        (1) Original (with BBoxes)\n",
    "        (2) White-Balanced (with BBoxes)\n",
    "        (3) CLAHE-Enhanced (with BBoxes)\n",
    "        (4) CLAHE + Gamma Correction (with BBoxes)\n",
    "    - Displays the filename as well.\n",
    "    \"\"\"\n",
    "    if not image_files or not annotation_files:\n",
    "        print(\"No images or annotations found!\")\n",
    "        return\n",
    "\n",
    "    # Select a random image\n",
    "    random_image_path = random.choice(image_files)\n",
    "    annotation_path = labels_path / (random_image_path.stem + \".txt\")\n",
    "\n",
    "    # If no annotation found, skip\n",
    "    if not annotation_path.exists():\n",
    "        print(f\"No annotation file found for {random_image_path.name}\")\n",
    "        return\n",
    "\n",
    "    # Load image with PIL\n",
    "    with Image.open(random_image_path) as img:\n",
    "        # Get size for YOLO coordinate parsing\n",
    "        w, h = img.size\n",
    "\n",
    "        # Get bounding boxes\n",
    "        bboxes = parse_yolo_annotations(annotation_path, w, h)\n",
    "\n",
    "        # -----------------------------\n",
    "        # # 1) Original with BBoxes\n",
    "        # # -----------------------------\n",
    "        # original_with_bboxes = draw_bounding_boxes(img, bboxes, color=\"red\")\n",
    "        #\n",
    "        # # -----------------------------\n",
    "        # # 2) White-Balanced\n",
    "        # # -----------------------------\n",
    "        # cv_bgr = pil_to_bgr(img)\n",
    "        # wb_bgr = white_balance_bgr(cv_bgr)\n",
    "        # wb_pil = bgr_to_pil(wb_bgr)\n",
    "        # wb_with_bboxes = draw_bounding_boxes(wb_pil, bboxes, color=\"blue\")\n",
    "\n",
    "        # -----------------------------\n",
    "        # 3) CLAHE-Enhanced\n",
    "        # -----------------------------\n",
    "        clahe_bgr = enhance_clahe_bgr(cv_bgr, clip_limit=2.0, grid_size=(8,8))\n",
    "        clahe_pil = bgr_to_pil(clahe_bgr)\n",
    "        clahe_with_bboxes = draw_bounding_boxes(clahe_pil, bboxes, color=\"green\")\n",
    "\n",
    "        # -----------------------------\n",
    "        # 4) CLAHE + Gamma Correction\n",
    "        # -----------------------------\n",
    "        clahe_gamma_bgr = gamma_correction_bgr(clahe_bgr, gamma=1.2)\n",
    "        clahe_gamma_pil = bgr_to_pil(clahe_gamma_bgr)\n",
    "        clahe_gamma_with_bboxes = draw_bounding_boxes(clahe_gamma_pil, bboxes, color=\"orange\")\n",
    "\n",
    "        # Display side by side\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(24, 6))\n",
    "        fig.suptitle(f\"File: {random_image_path.name}\", fontsize=16, y=1.05)\n",
    "\n",
    "        # # Original\n",
    "        # axes[0].imshow(original_with_bboxes)\n",
    "        # axes[0].set_title(\"Original\")\n",
    "        # axes[0].axis(\"off\")\n",
    "        #\n",
    "        # # White Balanced\n",
    "        # axes[1].imshow(wb_with_bboxes)\n",
    "        # axes[1].set_title(\"White-Balanced\")\n",
    "        # axes[1].axis(\"off\")\n",
    "        #\n",
    "        # # CLAHE Enhanced\n",
    "        # axes[2].imshow(clahe_with_bboxes)\n",
    "        # axes[2].set_title(\"CLAHE-Enhanced\")\n",
    "        # axes[2].axis(\"off\")\n",
    "\n",
    "        # CLAHE + Gamma\n",
    "        axes[3].imshow(clahe_gamma_with_bboxes)\n",
    "        axes[3].set_title(\"CLAHE + Gamma\")\n",
    "        axes[3].axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 6. EXECUTION\n",
    "# =============================================================================\n",
    "display_random_image_with_different_preprocessing()\n"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv_bgr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 253\u001B[0m\n\u001B[1;32m    248\u001B[0m         plt\u001B[38;5;241m.\u001B[39mshow()\n\u001B[1;32m    250\u001B[0m \u001B[38;5;66;03m# =============================================================================\u001B[39;00m\n\u001B[1;32m    251\u001B[0m \u001B[38;5;66;03m# 6. EXECUTION\u001B[39;00m\n\u001B[1;32m    252\u001B[0m \u001B[38;5;66;03m# =============================================================================\u001B[39;00m\n\u001B[0;32m--> 253\u001B[0m \u001B[43mdisplay_random_image_with_different_preprocessing\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[7], line 212\u001B[0m, in \u001B[0;36mdisplay_random_image_with_different_preprocessing\u001B[0;34m()\u001B[0m\n\u001B[1;32m    194\u001B[0m bboxes \u001B[38;5;241m=\u001B[39m parse_yolo_annotations(annotation_path, w, h)\n\u001B[1;32m    196\u001B[0m \u001B[38;5;66;03m# -----------------------------\u001B[39;00m\n\u001B[1;32m    197\u001B[0m \u001B[38;5;66;03m# # 1) Original with BBoxes\u001B[39;00m\n\u001B[1;32m    198\u001B[0m \u001B[38;5;66;03m# # -----------------------------\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    210\u001B[0m \u001B[38;5;66;03m# 3) CLAHE-Enhanced\u001B[39;00m\n\u001B[1;32m    211\u001B[0m \u001B[38;5;66;03m# -----------------------------\u001B[39;00m\n\u001B[0;32m--> 212\u001B[0m clahe_bgr \u001B[38;5;241m=\u001B[39m enhance_clahe_bgr(\u001B[43mcv_bgr\u001B[49m, clip_limit\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2.0\u001B[39m, grid_size\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m8\u001B[39m,\u001B[38;5;241m8\u001B[39m))\n\u001B[1;32m    213\u001B[0m clahe_pil \u001B[38;5;241m=\u001B[39m bgr_to_pil(clahe_bgr)\n\u001B[1;32m    214\u001B[0m clahe_with_bboxes \u001B[38;5;241m=\u001B[39m draw_bounding_boxes(clahe_pil, bboxes, color\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgreen\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'cv_bgr' is not defined"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-26T22:24:15.850838Z",
     "start_time": "2025-01-26T22:24:15.848659Z"
    }
   },
   "id": "28f8f5bee857b29f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "97c03d97b773305b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
